{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwKCupnjwIWS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#def generate_code_and_year(attributes):\n",
        " #   code = ''.join([str(int(attr * 10)) for attr in attributes])\n",
        "  #  year = int(sum(attributes) * 100)  # Adjusted logic for Year calculation\n",
        "   # return code, year\n",
        "\n",
        "\n",
        "#def predict_year_from_code(model, code, attributes):\n",
        "    # Assuming model.predict takes a 2D array as input\n",
        " #   input_data = pd.DataFrame([attributes], columns=features)\n",
        "  #  predicted_year = model.predict(input_data)[0][0]\n",
        "   # return predicted_year\n",
        "#data.info()"
      ],
      "metadata": {
        "id": "k-lIWxr0wv-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code_and_year(attributes):\n",
        "    # Use the actual 'Year' column values from the dataset to generate a realistic year\n",
        "    year = int(sum(attributes) / len(attributes))  # Average of attributes as a placeholder\n",
        "    code = ''.join([str(int(attr * 10)) for attr in attributes])\n",
        "    return code, year\n",
        "\n",
        "def predict_year_from_code(code, attributes):\n",
        "    # Use the actual model to predict the year based on the provided code and attributes\n",
        "    input_data = pd.DataFrame([attributes], columns=features)  # Create a DataFrame with the given attributes\n",
        "    input_data_scaled = scaler.transform(input_data)  # Scale the input data using the previously fitted scaler\n",
        "    predicted_year = model.predict(input_data_scaled)[0][0]  # Use the trained model for prediction\n",
        "    return predicted_year\n",
        "\n",
        "def generate_code_and_year(attributes):\n",
        "    # Scale the attributes using the fitted scaler\n",
        "    scaled_attributes = scaler.transform([attributes])\n",
        "\n",
        "    # Use the scaled attributes to generate a realistic year\n",
        "    year = int(model.predict(scaled_attributes)[0][0])\n",
        "\n",
        "    # Generate code using the scaled attributes\n",
        "    code = ''.join([str(int(attr * 10)) for attr in attributes])\n",
        "\n",
        "    return code, year\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ1K5ldiAYIQ",
        "outputId": "5b081ce2-aeed-4d43-aab2-e2bf9d435f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6120 entries, 0 to 6119\n",
            "Data columns (total 34 columns):\n",
            " #   Column                                      Non-Null Count  Dtype \n",
            "---  ------                                      --------------  ----- \n",
            " 0   Country/Territory                           6120 non-null   object\n",
            " 1   Code                                        6120 non-null   object\n",
            " 2   Year                                        6120 non-null   int64 \n",
            " 3   Meningitis                                  6120 non-null   int64 \n",
            " 4   Alzheimer's Disease and Other Dementias     6120 non-null   int64 \n",
            " 5   Parkinson's Disease                         6120 non-null   int64 \n",
            " 6   Nutritional Deficiencies                    6120 non-null   int64 \n",
            " 7   Malaria                                     6120 non-null   int64 \n",
            " 8   Drowning                                    6120 non-null   int64 \n",
            " 9   Interpersonal Violence                      6120 non-null   int64 \n",
            " 10  Maternal Disorders                          6120 non-null   int64 \n",
            " 11  HIV/AIDS                                    6120 non-null   int64 \n",
            " 12  Drug Use Disorders                          6120 non-null   int64 \n",
            " 13  Tuberculosis                                6120 non-null   int64 \n",
            " 14  Cardiovascular Diseases                     6120 non-null   int64 \n",
            " 15  Lower Respiratory Infections                6120 non-null   int64 \n",
            " 16  Neonatal Disorders                          6120 non-null   int64 \n",
            " 17  Alcohol Use Disorders                       6120 non-null   int64 \n",
            " 18  Self-harm                                   6120 non-null   int64 \n",
            " 19  Exposure to Forces of Nature                6120 non-null   int64 \n",
            " 20  Diarrheal Diseases                          6120 non-null   int64 \n",
            " 21  Environmental Heat and Cold Exposure        6120 non-null   int64 \n",
            " 22  Neoplasms                                   6120 non-null   int64 \n",
            " 23  Conflict and Terrorism                      6120 non-null   int64 \n",
            " 24  Diabetes Mellitus                           6120 non-null   int64 \n",
            " 25  Chronic Kidney Disease                      6120 non-null   int64 \n",
            " 26  Poisonings                                  6120 non-null   int64 \n",
            " 27  Protein-Energy Malnutrition                 6120 non-null   int64 \n",
            " 28  Road Injuries                               6120 non-null   int64 \n",
            " 29  Chronic Respiratory Diseases                6120 non-null   int64 \n",
            " 30  Cirrhosis and Other Chronic Liver Diseases  6120 non-null   int64 \n",
            " 31  Digestive Diseases                          6120 non-null   int64 \n",
            " 32  Fire, Heat, and Hot Substances              6120 non-null   int64 \n",
            " 33  Acute Hepatitis                             6120 non-null   int64 \n",
            "dtypes: int64(32), object(2)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lines(value):\n",
        "  for i in range(value):\n",
        "    print(\"======================================\")"
      ],
      "metadata": {
        "id": "sMpML_kF6XC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizing the data setfrom sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "IXf_jXsD_5ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "# Assuming you have a DataFrame 'data' with attributes, Code, and Year columns\n",
        "# Replace 'data.csv' with your actual dataset file or load your data accordingly\n",
        "data = pd.read_csv('/content/cause_of_deaths_new.csv')"
      ],
      "metadata": {
        "id": "5SbyaLtz6gZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['Meningitis', \"Alzheimer's Disease and Other Dementias\", \"Parkinson's Disease\", 'Nutritional Deficiencies', 'Malaria',\n",
        "            'Drowning', 'Interpersonal Violence', 'Maternal Disorders', 'HIV/AIDS', 'Drug Use Disorders']\n",
        "\n",
        "target = ['Year']"
      ],
      "metadata": {
        "id": "htaQmgYm6xW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase-I: Train ANN\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KvXOVtoF6yjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=10, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=10, activation='relu'))\n",
        "model.add(Dense(units=1, activation='linear'))"
      ],
      "metadata": {
        "id": "mAe9X66h610Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preivous One\n",
        "#model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "model.compile(optimizer='adam', loss='mse')\n"
      ],
      "metadata": {
        "id": "bQTDnKMt7SmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ubxq4td7Vhh",
        "outputId": "e989ff6a-04ab-42fe-af62-5cf90c5eccfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "153/153 [==============================] - 1s 3ms/step - loss: 4034081.0000 - val_loss: 3524038.7500\n",
            "Epoch 2/5\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 3407796.7500 - val_loss: 3390659.7500\n",
            "Epoch 3/5\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 3336975.5000 - val_loss: 3344900.5000\n",
            "Epoch 4/5\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 3304094.5000 - val_loss: 3319018.2500\n",
            "Epoch 5/5\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 3273027.2500 - val_loss: 3319263.2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cf9a6c28430>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase-II: Generate Code, Attributes, and Predict Year\n",
        "given_attributes_phase_01 = [0.2, 0.5, 0.1, 0.8, 0.3, 0.6, 0.9, 0.4, 0.7, 0.2]\n",
        "# Generate Code and Year based on given attributes\n",
        "\n",
        "generated_code_01, generated_year_01 = generate_code_and_year(given_attributes_phase_01)\n",
        "print(\"Phase-I [01]:\")\n",
        "print(\"Generated Code:\", generated_code_01)\n",
        "print(\"Generated Year:\", generated_year_01)\n",
        "\n",
        "\n",
        "lines(3)\n",
        "\n",
        "given_attributes_phase_02 = [1.5, 2.5, 0.5, 0.3, 0.3, 0.1, 3.2, 0.8, 1.2, 0.1]\n",
        "generated_code_02, generated_year_02 = generate_code_and_year(given_attributes_phase_02)\n",
        "print(\"Phase - I [02]\")\n",
        "print(\"Generated Code:\", generated_code_02)\n",
        "print(\"Generated Year:\", generated_year_02)\n"
      ],
      "metadata": {
        "id": "y98dKtVC7Wwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d640cd3-459a-4a74-bbec-08152bade0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 42ms/step\n",
            "Phase-I [01]:\n",
            "Generated Code: 2518369472\n",
            "Generated Year: 2\n",
            "======================================\n",
            "======================================\n",
            "======================================\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "Phase - I [02]\n",
            "Generated Code: 15255331328121\n",
            "Generated Year: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iVSB9AQJCMRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict Year based on Code and attributes\n",
        "predicted_year_phase_01 = predict_year_from_code(generated_code_01, given_attributes_phase_01)\n",
        "print(\"\\nPhase-II:\")\n",
        "print(\"Given Code:\", generated_code_01)\n",
        "print(\"Given Attributes:\", given_attributes_phase_01)\n",
        "print(\"Predicted Year:\", predicted_year_phase_01)\n",
        "\n",
        "lines(4)\n",
        "\n",
        "predicted_year_phase_02 = predict_year_from_code(generated_code_02, given_attributes_phase_02)\n",
        "print(\"\\nPhase-II:\")\n",
        "print(\"Given Code:\", generated_code_02)\n",
        "print(\"Given Attributes:\", given_attributes_phase_02)\n",
        "print(\"Predicted Year:\", predicted_year_phase_02)\n",
        "\n",
        "\n",
        "# Predict Year based on Code and attributes\n",
        "\n",
        "#predicted_year_phase_01 = predict_year_from_code(model, generated_code_01, given_attributes_phase_01)\n",
        "#print(\"\\nPhase-II:\")\n",
        "#print(\"Given Code:\", generated_code_01)\n",
        "#print(\"Given Attributes:\", given_attributes_phase_01)\n",
        "#print(\"Predicted Year:\", predicted_year_phase_01)\n",
        "\n",
        "#lines(4)\n",
        "\n",
        "#predicted_year_phase_02 = predict_year_from_code(model, generated_code_02, given_attributes_phase_02)\n",
        "#print(\"\\nPhase-II:\")\n",
        "#print(\"Given Code:\", generated_code_02)\n",
        "#print(\"Given Attributes:\", given_attributes_phase_02)\n",
        "#print(\"Predicted Year:\", predicted_year_phase_02)\n"
      ],
      "metadata": {
        "id": "ATT-XtSQ7hMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65c47e1-25fb-4657-a6f2-fe3300b6cc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "\n",
            "Phase-II:\n",
            "Given Code: 2518369472\n",
            "Given Attributes: [0.2, 0.5, 0.1, 0.8, 0.3, 0.6, 0.9, 0.4, 0.7, 0.2]\n",
            "Predicted Year: 2.56083\n",
            "======================================\n",
            "======================================\n",
            "======================================\n",
            "======================================\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "\n",
            "Phase-II:\n",
            "Given Code: 15255331328121\n",
            "Given Attributes: [1.5, 2.5, 0.5, 0.3, 0.3, 0.1, 3.2, 0.8, 1.2, 0.1]\n",
            "Predicted Year: 2.5608382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Function to generate Code and Year based on attributes\n",
        "def generate_code_and_year(attributes, model, scaler):\n",
        "    # Scale the attributes using the fitted scaler\n",
        "    scaled_attributes = scaler.transform([attributes])\n",
        "\n",
        "    # Use the scaled attributes to generate a realistic year\n",
        "    year = int(model.predict(scaled_attributes)[0][0])\n",
        "\n",
        "    # Generate code using the scaled attributes\n",
        "    code = ''.join([str(int(attr * 10)) for attr in attributes])\n",
        "\n",
        "    return code, year\n",
        "\n",
        "# Function to predict Year based on Code and attributes\n",
        "def predict_year_from_code(code, attributes, model, scaler):\n",
        "    # Scale the attributes using the fitted scaler\n",
        "    scaled_attributes = scaler.transform([attributes])\n",
        "\n",
        "    # Use the trained model to predict the year based on the provided code and scaled attributes\n",
        "    predicted_year = int(model.predict(scaled_attributes)[0][0])\n",
        "    return predicted_year\n",
        "\n",
        "def lines(value):\n",
        "    for i in range(value):\n",
        "        print(\"======================================\")\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/cause_of_deaths_new.csv')\n",
        "\n",
        "# Define features and target\n",
        "features = ['Meningitis', \"Alzheimer's Disease and Other Dementias\", \"Parkinson's Disease\", 'Nutritional Deficiencies', 'Malaria',\n",
        "            'Drowning', 'Interpersonal Violence', 'Maternal Disorders', 'HIV/AIDS', 'Drug Use Disorders']\n",
        "\n",
        "target = ['Year']\n",
        "\n",
        "# Phase-I: Train ANN\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=10, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Phase-II: Generate Code, Attributes, and Predict Year\n",
        "given_attributes_phase_01 = [0.2, 0.5, 0.1, 0.8, 0.3, 0.6, 0.9, 0.4, 0.7, 0.2]\n",
        "# Generate Code and Year based on given attributes\n",
        "generated_code_01, generated_year_01 = generate_code_and_year(given_attributes_phase_01, model, scaler)\n",
        "print(\"Phase-I [01]:\")\n",
        "print(\"Generated Code:\", generated_code_01)\n",
        "print(\"Generated Year:\", generated_year_01)\n",
        "\n",
        "lines(3)\n",
        "\n",
        "given_attributes_phase_02 = [1.5, 2.5, 0.5, 0.3, 0.3, 0.1, 3.2, 0.8, 1.2, 0.1]\n",
        "generated_code_02, generated_year_02 = generate_code_and_year(given_attributes_phase_02, model, scaler)\n",
        "print(\"Phase - I [02]\")\n",
        "print(\"Generated Code:\", generated_code_02)\n",
        "print(\"Generated Year:\", generated_year_02)\n",
        "\n",
        "# Predict Year based on Code and attributes\n",
        "predicted_year_phase_01 = predict_year_from_code(generated_code_01, given_attributes_phase_01, model, scaler)\n",
        "print(\"\\nPhase-II:\")\n",
        "print(\"Given Code:\", generated_code_01)\n",
        "print(\"Given Attributes:\", given_attributes_phase_01)\n",
        "print(\"Predicted Year:\", predicted_year_phase_01)\n",
        "\n",
        "lines(4)\n",
        "\n",
        "predicted_year_phase_02 = predict_year_from_code(generated_code_02, given_attributes_phase_02, model, scaler)\n",
        "print(\"\\nPhase-II:\")\n",
        "print(\"Given Code:\", generated_code_02)\n",
        "print(\"Given Attributes:\", given_attributes_phase_02)\n",
        "print(\"Predicted Year:\", predicted_year_phase_02)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNwl07hTA_Uw",
        "outputId": "247e3014-1a88-4934-d2f5-87bb788a59e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "153/153 [==============================] - 1s 2ms/step - loss: 4017180.5000 - val_loss: 4017208.5000\n",
            "Epoch 2/5\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 4014610.5000 - val_loss: 4013864.2500\n",
            "Epoch 3/5\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 4010466.2500 - val_loss: 4008947.5000\n",
            "Epoch 4/5\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 4004767.0000 - val_loss: 4002516.5000\n",
            "Epoch 5/5\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 3997576.0000 - val_loss: 3994625.7500\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Phase-I [01]:\n",
            "Generated Code: 2518369472\n",
            "Generated Year: 5\n",
            "======================================\n",
            "======================================\n",
            "======================================\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Phase - I [02]\n",
            "Generated Code: 15255331328121\n",
            "Generated Year: 5\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "\n",
            "Phase-II:\n",
            "Given Code: 2518369472\n",
            "Given Attributes: [0.2, 0.5, 0.1, 0.8, 0.3, 0.6, 0.9, 0.4, 0.7, 0.2]\n",
            "Predicted Year: 5\n",
            "======================================\n",
            "======================================\n",
            "======================================\n",
            "======================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n",
            "\n",
            "Phase-II:\n",
            "Given Code: 15255331328121\n",
            "Given Attributes: [1.5, 2.5, 0.5, 0.3, 0.3, 0.1, 3.2, 0.8, 1.2, 0.1]\n",
            "Predicted Year: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/cause_of_deaths_new.csv')\n",
        "\n",
        "# Select 10 features and the target variable ('Year')\n",
        "selected_features = ['Meningitis', \"Alzheimer's Disease and Other Dementias\", \"Parkinson's Disease\", \"Nutritional Deficiencies\", \"Malaria\",\n",
        "                      \"Drowning\", \"Interpersonal Violence\", \"Maternal Disorders\", 'HIV/AIDS', \"Drug Use Disorders\", 'Year']\n",
        "\n",
        "data_selected = data[selected_features]\n",
        "\n",
        "# Data preprocessing\n",
        "X = data_selected.drop(['Year'], axis=1)\n",
        "y = data_selected['Year']\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# For simplicity, assuming all columns are numerical\n",
        "label_encoder = LabelEncoder()\n",
        "X_encoded = X.apply(label_encoder.fit_transform)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Convert predictions to integer values (assuming 'Year' is discrete)\n",
        "y_pred_int = np.round(y_pred).astype(int)\n",
        "\n",
        "# Evaluate the accuracy (optional)\n",
        "accuracy = accuracy_score(y_test, y_pred_int)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Define user input features\n",
        "user_input_feature1 = 0.5  # Replace with the actual value\n",
        "user_input_feature2 = 1.2  # Replace with the actual value\n",
        "user_input_feature3 = 0.8  # Replace with the actual value\n",
        "user_input_feature4 = 1.4\n",
        "user_input_feature5 = 5.8\n",
        "user_input_feature6 = 7.6\n",
        "user_input_feature7 = 2.6\n",
        "user_input_feature8 = 1.2\n",
        "user_input_feature9 = 4.2\n",
        "user_input_feature10 = 0.1\n",
        "# ...\n",
        "\n",
        "# Test the model with user input\n",
        "user_input = pd.DataFrame({'Meningitis': [user_input_feature1],\n",
        "                            \"Alzheimer's Disease and Other Dementias\": [user_input_feature2],\n",
        "                            \"Parkinson's Disease\": [user_input_feature3],\n",
        "                            'Nutritional Deficiencies': [user_input_feature4],\n",
        "                            'Malaria': [user_input_feature5],\n",
        "                            'Drowning': [user_input_feature6],\n",
        "                            \"Interpersonal Violence\": [user_input_feature7],\n",
        "                            \"Maternal Disorders\": [user_input_feature8],\n",
        "                            \"HIV/AIDS\": [user_input_feature9],\n",
        "                            'Drug Use Disorders': [user_input_feature10]})\n",
        "\n",
        "\n",
        "# Encode user input\n",
        "user_input_encoded = user_input.apply(label_encoder.transform)\n",
        "\n",
        "# Scale user input\n",
        "user_input_scaled = scaler.transform(user_input_encoded)\n",
        "\n",
        "# Predict the year\n",
        "predicted_year = model.predict(user_input_scaled)\n",
        "print(f'Predicted Year: {int(predicted_year[0][0])}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f_aAkjcBPeq",
        "outputId": "70c6665d-e4f0-4db6-bffd-3031ca15395f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "153/153 [==============================] - 1s 1ms/step - loss: 3903658.2500\n",
            "Epoch 2/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 2596949.0000\n",
            "Epoch 3/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 910034.9375\n",
            "Epoch 4/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 590005.6250\n",
            "Epoch 5/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 478552.6875\n",
            "Epoch 6/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 366666.9062\n",
            "Epoch 7/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 264468.3750\n",
            "Epoch 8/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 176207.7500\n",
            "Epoch 9/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 111135.1875\n",
            "Epoch 10/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 73568.7266\n",
            "Epoch 11/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 52410.6562\n",
            "Epoch 12/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 38479.7773\n",
            "Epoch 13/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 29114.9609\n",
            "Epoch 14/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 22533.4492\n",
            "Epoch 15/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 17860.6914\n",
            "Epoch 16/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 14341.6719\n",
            "Epoch 17/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 11686.9902\n",
            "Epoch 18/100\n",
            "153/153 [==============================] - 0s 985us/step - loss: 9626.7656\n",
            "Epoch 19/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 8071.0645\n",
            "Epoch 20/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 6937.0391\n",
            "Epoch 21/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 6012.3496\n",
            "Epoch 22/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 5279.8076\n",
            "Epoch 23/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 4680.6523\n",
            "Epoch 24/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 4155.8901\n",
            "Epoch 25/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 3707.5254\n",
            "Epoch 26/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 3331.2747\n",
            "Epoch 27/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 3007.6401\n",
            "Epoch 28/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 2721.1760\n",
            "Epoch 29/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 2462.0798\n",
            "Epoch 30/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 2227.1099\n",
            "Epoch 31/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 2010.6799\n",
            "Epoch 32/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 1823.7479\n",
            "Epoch 33/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 1659.3628\n",
            "Epoch 34/100\n",
            "153/153 [==============================] - 0s 979us/step - loss: 1469.0265\n",
            "Epoch 35/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 1313.0503\n",
            "Epoch 36/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 1157.0334\n",
            "Epoch 37/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 1028.8938\n",
            "Epoch 38/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 926.4008\n",
            "Epoch 39/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 832.3978\n",
            "Epoch 40/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 756.8395\n",
            "Epoch 41/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 662.2068\n",
            "Epoch 42/100\n",
            "153/153 [==============================] - 1s 4ms/step - loss: 599.0263\n",
            "Epoch 43/100\n",
            "153/153 [==============================] - 0s 3ms/step - loss: 539.0845\n",
            "Epoch 44/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 495.4789\n",
            "Epoch 45/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 444.0921\n",
            "Epoch 46/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 398.7311\n",
            "Epoch 47/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 361.5852\n",
            "Epoch 48/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 333.3596\n",
            "Epoch 49/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 294.2031\n",
            "Epoch 50/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 265.8369\n",
            "Epoch 51/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 237.6263\n",
            "Epoch 52/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 214.9719\n",
            "Epoch 53/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 195.2561\n",
            "Epoch 54/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 178.1823\n",
            "Epoch 55/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 159.5438\n",
            "Epoch 56/100\n",
            "153/153 [==============================] - 0s 3ms/step - loss: 143.7168\n",
            "Epoch 57/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 133.0220\n",
            "Epoch 58/100\n",
            "153/153 [==============================] - 0s 979us/step - loss: 125.6598\n",
            "Epoch 59/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 116.8640\n",
            "Epoch 60/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 109.4294\n",
            "Epoch 61/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 110.7275\n",
            "Epoch 62/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 102.0881\n",
            "Epoch 63/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 99.4738\n",
            "Epoch 64/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 97.7822\n",
            "Epoch 65/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 92.9080\n",
            "Epoch 66/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 90.9759\n",
            "Epoch 67/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 88.0094\n",
            "Epoch 68/100\n",
            "153/153 [==============================] - 0s 990us/step - loss: 85.5896\n",
            "Epoch 69/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 86.1431\n",
            "Epoch 70/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 80.5669\n",
            "Epoch 71/100\n",
            "153/153 [==============================] - 0s 981us/step - loss: 74.7544\n",
            "Epoch 72/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 74.8707\n",
            "Epoch 73/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 76.3242\n",
            "Epoch 74/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 74.2990\n",
            "Epoch 75/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 73.3574\n",
            "Epoch 76/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 70.9858\n",
            "Epoch 77/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 70.8826\n",
            "Epoch 78/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 70.1797\n",
            "Epoch 79/100\n",
            "153/153 [==============================] - 0s 994us/step - loss: 68.3106\n",
            "Epoch 80/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 69.6558\n",
            "Epoch 81/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 70.7799\n",
            "Epoch 82/100\n",
            "153/153 [==============================] - 0s 984us/step - loss: 69.6178\n",
            "Epoch 83/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 73.6594\n",
            "Epoch 84/100\n",
            "153/153 [==============================] - 0s 965us/step - loss: 70.0220\n",
            "Epoch 85/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 69.8653\n",
            "Epoch 86/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 69.8121\n",
            "Epoch 87/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.7584\n",
            "Epoch 88/100\n",
            "153/153 [==============================] - 0s 2ms/step - loss: 67.2857\n",
            "Epoch 89/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 69.1156\n",
            "Epoch 90/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.3930\n",
            "Epoch 91/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 69.3557\n",
            "Epoch 92/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.3504\n",
            "Epoch 93/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.5271\n",
            "Epoch 94/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.2367\n",
            "Epoch 95/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 68.9530\n",
            "Epoch 96/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 70.3780\n",
            "Epoch 97/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 65.4462\n",
            "Epoch 98/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 67.2812\n",
            "Epoch 99/100\n",
            "153/153 [==============================] - 0s 991us/step - loss: 72.3218\n",
            "Epoch 100/100\n",
            "153/153 [==============================] - 0s 1ms/step - loss: 67.2658\n",
            "39/39 [==============================] - 0s 913us/step\n",
            "Accuracy: 4.58%\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "Predicted Year: 2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yq6HBXQIO_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5BDDGs1CP4Cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}